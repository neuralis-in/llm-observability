{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47072c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests   #newly added dependency\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "from google import genai\n",
    "from tqdm import tqdm     #newly added dependency\n",
    "from aiobs.evals import HallucinationDetectionEval, EvalInput\n",
    "from aiobs import observer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0522975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations (defaults)\n",
    "JUDGE_MODEL = \"gemini-2.5-flash\"               # judge model: gemini-flash-2.5 as of now  \n",
    "SAMPLE_SIZE = None                  # None = full dataset, or set e.g., 200\n",
    "DATA_URL = \"https://raw.githubusercontent.com/RUCAIBox/HaluEval/main/data/qa_data.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4180bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================HELPER FUNCTIONS=======================\n",
    "\n",
    "\n",
    "\n",
    "def load_halueval_qa(url: str) -> List[Dict]:\n",
    "    \"\"\"Fetch HaluEvalQA dataset from GitHub raw using HTTP GET.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    lines = response.text.strip().split(\"\\n\")\n",
    "    return [json.loads(line) for line in lines]\n",
    "\n",
    "\n",
    "def run_benchmark(\n",
    "    evaluator: HallucinationDetectionEval,\n",
    "    data: List[Dict],\n",
    "    sample_size: int = None,\n",
    ") -> Dict:\n",
    "    \"\"\"Run benchmark and return metrics.\"\"\"\n",
    "    results = {\n",
    "        \"hallucinated_detected\": 0,  # True positives\n",
    "        \"hallucinated_missed\": 0,    # False negatives\n",
    "        \"correct_passed\": 0,         # True negatives\n",
    "        \"correct_flagged\": 0,        # False positives\n",
    "    }\n",
    "    \n",
    "    samples = data[:sample_size] if sample_size else data\n",
    "    \n",
    "    for sample in tqdm(samples, desc=\"Checking on benchmarks\"):\n",
    "        knowledge = sample[\"knowledge\"]\n",
    "        question = sample[\"question\"]\n",
    "        \n",
    "        # Test hallucinated answer (should fail)\n",
    "        result_hallucinated = evaluator.evaluate(EvalInput(\n",
    "            user_input=question,\n",
    "            model_output=sample[\"hallucinated_answer\"],\n",
    "            context={\"documents\": [knowledge]},\n",
    "        ))\n",
    "        \n",
    "        if result_hallucinated.failed:\n",
    "            results[\"hallucinated_detected\"] += 1\n",
    "        else:\n",
    "            results[\"hallucinated_missed\"] += 1\n",
    "        \n",
    "        # Test correct answer (should pass)\n",
    "        result_correct = evaluator.evaluate(EvalInput(\n",
    "            user_input=question,\n",
    "            model_output=sample[\"right_answer\"],\n",
    "            context={\"documents\": [knowledge]},\n",
    "        ))\n",
    "        \n",
    "        if result_correct.passed:\n",
    "            results[\"correct_passed\"] += 1\n",
    "        else:\n",
    "            results[\"correct_flagged\"] += 1\n",
    "    \n",
    "    # Calculate metrics\n",
    "    total = len(samples) * 2\n",
    "    accuracy = (results[\"hallucinated_detected\"] + results[\"correct_passed\"]) / total\n",
    "    \n",
    "    precision = results[\"hallucinated_detected\"] / (\n",
    "        results[\"hallucinated_detected\"] + results[\"correct_flagged\"]\n",
    "    ) if (results[\"hallucinated_detected\"] + results[\"correct_flagged\"]) > 0 else 0\n",
    "    \n",
    "    recall = results[\"hallucinated_detected\"] / (\n",
    "        results[\"hallucinated_detected\"] + results[\"hallucinated_missed\"]\n",
    "    ) if (results[\"hallucinated_detected\"] + results[\"hallucinated_missed\"]) > 0 else 0\n",
    "    \n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        \"raw_counts\": results,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"total_samples\": len(samples),\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0436204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_gemini(client: genai.Client, model: str):\n",
    "    \"\"\"Verify Gemini API key and model availability by sending a minimal request.\"\"\"\n",
    "    try:\n",
    "        client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=\"ping\", \n",
    "        )\n",
    "        print(f\"[OK] Gemini successfully initialized with model: {model}\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"\\n[ERROR] Gemini initialization failed!\")\n",
    "        print(\"Reason:\", str(e))\n",
    "        print(\"\\nPossible causes:\")\n",
    "        print(\" - GEMINI_API_KEY missing or invalid\")\n",
    "        print(\" - Model name incorrect\")\n",
    "        print(\" - Network issue\")\n",
    "        print(\" - Permissions not enabled for this model\\n\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b097ff32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    observer.observe(\"Hallucination Benchmarking\")\n",
    "    \n",
    "    dataset = load_halueval_qa(DATA_URL)\n",
    "\n",
    "    client = genai.Client()\n",
    "\n",
    "    if not verify_gemini(client, JUDGE_MODEL):\n",
    "        observer.end()\n",
    "        return\n",
    "    \n",
    "    evaluation_model = HallucinationDetectionEval.with_gemini(client=client , model=JUDGE_MODEL)\n",
    "\n",
    "    metrics = run_benchmark(evaluator=evaluation_model , data=dataset , sample_size=SAMPLE_SIZE)\n",
    "\n",
    "    print(\"\\n=== Metrics ===\")\n",
    "    for key, value in metrics.items():\n",
    "        if isinstance(value, dict):\n",
    "            print(f\"{key}:\")\n",
    "            for k, v in value.items():\n",
    "                print(f\"  {k}: {v}\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")\n",
    "\n",
    "\n",
    "    observer.end()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c9c807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from getpass import getpass\n",
    "load_dotenv()\n",
    "SAMPLE_SIZE = 10\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not ( gemini_api_key or gemini_api_key.strip() == \"\" ):\n",
    "    gemini_api_key = getpass(\"Enter your Gemini API key: \")\n",
    "os.environ[\"GEMINI_API_KEY\"] = gemini_api_key\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
